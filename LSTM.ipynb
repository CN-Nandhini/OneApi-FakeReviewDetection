{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810b8c00-0661-4401-bf52-83f5738782ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/inteloneapi/intelpython/latest/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e72b42-fc0f-4afa-b691-d2af8e2cd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420fc491-1964-451e-8c6b-b70963da32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4c0b7-e642-4d3b-94f4-4e30a3db36f7",
   "metadata": {},
   "source": [
    "#  Loading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc34c58b-f539-4513-87de-692b186c52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43bad043-c1d3-49c8-ac94-1ae25b188f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc31199-e7a2-46fe-8444-1a12d9c67d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80b61ed-0b2a-478a-8571-ada2dac1514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['text_'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e901372-4938-404a-8ea2-a5d596bd7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes = [\"CG\", \"OR\"]\n",
    "\n",
    "df['label'] = df['label'].apply(data_classes.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80e2aaa-7349-4971-b858-be542cb1e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>love well made sturdi comfort i love veri pretti</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>love great upgrad origin i 've mine coupl year</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>thi pillow save back i love look feel pillow</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>miss inform use great product price i</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>veri nice set good qualiti we set two month</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating  label  \\\n",
       "0  Home_and_Kitchen_5     5.0      0   \n",
       "1  Home_and_Kitchen_5     5.0      0   \n",
       "2  Home_and_Kitchen_5     5.0      0   \n",
       "3  Home_and_Kitchen_5     1.0      0   \n",
       "4  Home_and_Kitchen_5     5.0      0   \n",
       "\n",
       "                                              text_  length  \n",
       "0  love well made sturdi comfort i love veri pretti      48  \n",
       "1    love great upgrad origin i 've mine coupl year      46  \n",
       "2      thi pillow save back i love look feel pillow      44  \n",
       "3             miss inform use great product price i      37  \n",
       "4       veri nice set good qualiti we set two month      43  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccd5093-01df-4ca4-9593-3c8264110587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db0b0d-b63c-4961-bfd8-4d82ad172510",
   "metadata": {},
   "source": [
    "#  Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc973d3f-14b2-4b7d-bd5a-5589d2d5c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train, review_test, label_train, label_test = train_test_split(df['text_'],df['label'],test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1302dc51-6ab1-48b1-915f-ffbd809c46bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (review_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16524b06-e343-4431-af36-3fe26d405f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37303    the bag excel pocket wide tight i need small s...\n",
       "27745    i love seri one i pre-ord autobuy jenna bennet...\n",
       "2766     i realli love steam cloth iron it 's sooo much...\n",
       "24057                 i like just n't find good first book\n",
       "34720    thi huge pack fun kid love my son love one i b...\n",
       "                               ...                        \n",
       "35069    cute mlp plush it 's soft cuddli my daughter l...\n",
       "22802    we use coupl year i continu use long i the pla...\n",
       "10406    superb finish seriou photographi tool thi came...\n",
       "25824    love sarcasm banter blake jennif howev i could...\n",
       "21160    we older dog need joint they love tast take pr...\n",
       "Name: text_, Length: 26280, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0011c9-798a-4620-9e7a-911e65d41b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/u188274/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9659f-07ed-405f-aae7-2fd9d9bb046a",
   "metadata": {},
   "source": [
    "# Doc2Vec Model to build vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d4d583-7fb4-4925-a1be-e62947280936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.84 s, sys: 72.2 ms, total: 5.91 s\n",
      "Wall time: 5.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#The Doc2Vec model takes 'tagged_documents'\n",
    "\n",
    "tagged_tr = [TaggedDocument(words=word_tokenize(_d.lower()),\\\n",
    "tags=[str(i)]) for i, _d in enumerate(review_train)]\n",
    "\n",
    "# tagged_val = [TaggedDocument(words=word_tokenize(_d.lower()),\\\n",
    "# tags=[str(i)]) for i, _d in enumerate(val_data.REVIEW_TEXT)]\n",
    "\n",
    "tagged_test = [TaggedDocument(words=word_tokenize(_d.lower()),\\\n",
    "tags=[str(i)]) for i, _d in enumerate(review_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1982889f-60ef-465e-a1a8-ab0e87d9bd35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2vec.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "603883b6-56de-4885-987c-68d46e291367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.57 s, sys: 32.3 ms, total: 5.6 s\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Instantiate the model\n",
    "model = Doc2Vec(vector_size=60, \n",
    "                window=5, \n",
    "                alpha=.025, #initial learning rate\n",
    "                min_alpha=0.00025, #learning rate drops linearly to this\n",
    "                min_count=2, #ignores all words with total frequency lower than this.\n",
    "                dm =1, #algorithm 1=distributed memory\n",
    "                workers=16)#cores to use\n",
    "\n",
    "#build the vocab on the training data\n",
    "model.build_vocab(tagged_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b00e7eca-0d18-471b-94e4-6e1c2772be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26280 5 26280\n"
     ]
    }
   ],
   "source": [
    "print (model.corpus_count, model.epochs, len(tagged_tr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503573f-e28f-48e3-acc2-f96fbec4e37a",
   "metadata": {},
   "source": [
    "# Training Doc2VecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf4b9395-8c18-4fec-8ede-1c852b6f1a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 44s, sys: 2min 29s, total: 15min 14s\n",
      "Wall time: 8min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 15\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_tr,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a472688-8e2e-4271-964c-e677b2ef59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets\n",
    "model.save(\"doc2vec.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c546f0-6c51-4a5e-bc85-d8f41159811e",
   "metadata": {},
   "source": [
    "# Finding semantically similar words using Doc2VecModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e207845-89a1-4531-af5f-4c8d94b09883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"nice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9345c-c2e1-46a4-9bac-5b80216d5438",
   "metadata": {},
   "source": [
    "#  Tokenize the corpus using Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca025796-ca76-4c6b-b3ab-2fbdb6024cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input,concatenate,Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D,Bidirectional, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977c6f2-c438-4957-9b25-eed5d7d495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_train)\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Vocab Size is ',vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4ef64-58c8-43d0-91ed-d77a65e47bb7",
   "metadata": {},
   "source": [
    "\n",
    "# ** Building the embeddings using the vocabulary the corpus using Tokenizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de769d1-2077-41d9-a733-354f4f003b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 60))\n",
    "for i, row in review_train.items():\n",
    "    #print (i, row)\n",
    "    if i >= vocab_size - 1:\n",
    "        continue\n",
    "    if (str(i) in model.docvecs):\n",
    "        embedding = model.docvecs[str(i)]\n",
    "        if embedding is not None:\n",
    "            embedding_matrix[i+1] = embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cfe87-679f-4a10-8635-72bfc68b779f",
   "metadata": {},
   "source": [
    "#  Converting the tokens to sequences and padding them to sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c810ce1-6877-4827-a32b-e662c9469385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time \n",
    "SEQUENCE_LENGTH = 100\n",
    "x_data = pad_sequences(tokenizer.texts_to_sequences(review_train) , maxlen = SEQUENCE_LENGTH)\n",
    "y_data = label_train\n",
    "y_data = y_data.values.reshape(-1,1)\n",
    "\n",
    "# vx_data = pad_sequences(tokenizer.texts_to_sequences(val_data.REVIEW_TEXT) , maxlen = SEQUENCE_LENGTH)\n",
    "# vy_data = val_data.LABEL\n",
    "# vy_data = vy_data.values.reshape(-1,1)\n",
    "\n",
    "testx = pad_sequences(tokenizer.texts_to_sequences(review_test) , maxlen = SEQUENCE_LENGTH)\n",
    "testy = label_test\n",
    "testy = testy.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d25b0c-9342-4905-af9a-8272eddc7437",
   "metadata": {},
   "source": [
    "#  Building the Model using LSTM and OneDNN is enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541929b-7b8d-4ea2-a3d0-41bfdff16d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input = Input(shape=(SEQUENCE_LENGTH,)) \n",
    "                   \n",
    "emb=Embedding(vocab_size,60,weights=[embedding_matrix],input_length=SEQUENCE_LENGTH)(nlp_input)\n",
    " # layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "\n",
    "nlp_out = LSTM(64)(emb)                         \n",
    "classifier1 = Dense(128, activation='relu')(nlp_out) \n",
    "dropout = Dropout(0.2)(classifier1) \n",
    "classifier2 = Dense(32, activation='relu')(dropout) \n",
    "output = Dense(1, activation='sigmoid')(classifier2) \n",
    "\n",
    "model = Model(inputs=[nlp_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ab9cc-9dd3-4cfd-a9b7-723c102cebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dce141-d74b-4597-8960-fb18f8dfa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "history = model.fit(x_data,y_data,batch_size=32,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4de0a-8afc-4e79-be56-b80b04530794",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(testx, testy)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(testx[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "994a61b8-d004-445f-9798-5e62019c1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......embedding\n",
      ".........vars\n",
      "............0\n",
      "......input_layer\n",
      ".........vars\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-03-23 04:03:53           64\n",
      "variables.h5                                   2023-03-23 04:03:53     17990208\n",
      "config.json                                    2023-03-23 04:03:53         3766\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c0dca-1505-4f8c-b872-abd9c4917116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f809b7a-cbe9-4dfa-91ce-0fb98363ab07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (IntelÂ® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
